{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d1511e42-9d6e-41b7-9fbe-656ea109b87d","cell_type":"markdown","source":["# Train GNN-Siamese model\n","\n","Uses Optuna hyper-parameter optmization for finding the best GNN-Siamese architecture.\n","Train and save best model.\n","\n","--------------------"],"metadata":{"id":"d1511e42-9d6e-41b7-9fbe-656ea109b87d"}},{"id":"e135e71b-f39a-46e5-9977-266c67dc3be7","cell_type":"markdown","source":["### Imports"],"metadata":{"id":"e135e71b-f39a-46e5-9977-266c67dc3be7"}},{"cell_type":"code","source":["import os\n","from src.models.GNN_Siamese import *\n","import optuna"],"metadata":{"id":"2EhZUtVt79fd","executionInfo":{"status":"ok","timestamp":1738937690860,"user_tz":180,"elapsed":11912,"user":{"displayName":"Juliana Vizotto","userId":"07160986872486061354"}}},"id":"2EhZUtVt79fd","execution_count":6,"outputs":[]},{"id":"ecab7f1f-9aaf-436e-9e9a-aa03590f5b6e","cell_type":"markdown","source":["---------------\n","\n","### Define the hyper-parameters range for Optuna trial\n"],"metadata":{"id":"ecab7f1f-9aaf-436e-9e9a-aa03590f5b6e"}},{"cell_type":"code","source":["num_node_features = 13\n","n_graphs = [20,40,60]\n","num_layers=[1,3] #Number of layers suggest for optuna trial. num_layers[0]:min, num_layers[1]:max\n","batch_size=[10,20,30,40,50] #Sizes of batch sizes suggest for optuna trial\n","lr=[1e-4, 1e-2] #Learning rate suggest for optuna trial. num_layers[0]:min, num_layers[1]:max\n","activation_name=[torch.nn.Tanh(), torch.nn.ReLU(), torch.nn.Sigmoid()] #Activation functions suggest for optuna trial.\n","criterion_name=[torch.nn.MSELoss(), torch.nn.L1Loss()] #Loss functions suggest for optuna trial.\n","num_epochs=[5,50] # Number of epochs suggest for optuna trial\n","gat_heads=[1,2,3,4,5] #Number of gat heads suggest for optuna trial\n","readout_layer=[nn.global_mean_pool,nn.global_max_pool,nn.global_add_pool] #Pooling layers suggest for optuna trial.\n","num_trails=50 #Number of trials optuna should run\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"2g52apBY8FiH","executionInfo":{"status":"ok","timestamp":1738937724539,"user_tz":180,"elapsed":45,"user":{"displayName":"Juliana Vizotto","userId":"07160986872486061354"}}},"id":"2g52apBY8FiH","execution_count":8,"outputs":[]},{"id":"59b871d8-c5c9-40a4-a31e-9c427e48826f","cell_type":"markdown","source":["### Define auxiliar functions\n","\n","- Optuna objective trial: runs hyper-parameter optmization given the defined ranges above\n","- GNN-Siamese training function: train the full GNN-Siamese model with the parameters for the best model returned by optuna optimization\n","- Prepare training data: generator fited torch geometric DataLoader for training"],"metadata":{"id":"59b871d8-c5c9-40a4-a31e-9c427e48826f"}},{"cell_type":"code","source":["def objective(trial,num_layers,n_graphs,batch_size,lr,activation_name,criterion_name,num_epochs,gat_heads,readout_layer,num_node_features):#,\n","             #train_loader1, val_loader1,train_loader2, val_loader2):\n","    # Suggest hyperparameters\n","    num_node_features = num_node_features\n","    n_graphs = trial.suggest_categorical('n_graphs', n_graphs)\n","    num_layers = trial.suggest_int('num_conv_hidden_layers', num_layers[0], num_layers[1])\n","    conv_hidden_channels = []\n","    min_val = 9\n","    for i in range(num_layers):\n","        # Suggest a decreasing value for each subsequent layer\n","        max_val = 12 if i == 0 else conv_hidden_channels[-1] - 1\n","        if max_val < min_val:\n","            raise ValueError(\"Cannot generate strictly decreasing channels within the range.\")\n","        conv_hidden_channels.append(trial.suggest_int(f'conv_hidden_channels_{i}', min_val, max_val))\n","        min_val = min_val - 2\n","\n","    batch_size = trial.suggest_categorical('batch_size', batch_size)\n","    lr = trial.suggest_loguniform('lr', lr[0], lr[1])\n","    activation_name = trial.suggest_categorical('activation', activation_name)\n","    criterion = trial.suggest_categorical('criterion', criterion_name)\n","    num_epochs = trial.suggest_int('num_epochs', num_epochs[0], num_epochs[1])\n","    gat_heads = trial.suggest_categorical('gat_heads', gat_heads)\n","    readout_layer = trial.suggest_categorical('readout_layer',readout_layer)\n","\n","    print(batch_size)\n","\n","    spearman_corrcoef = DifferentialSpearmanCorrCoef(num_outputs=batch_size)\n","\n","    #Instanciate Model\n","    model = GNNSiamese(conv_hidden_channels=conv_hidden_channels,\n","                           activation=activation_name,\n","                           batch_size=batch_size,\n","                           readout_layer=readout_layer,\n","                           num_node_features=num_node_features,\n","                           n_graphs=n_graphs,\n","                           gat_heads=gat_heads).to(device)\n","\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    train_loader1, val_loader1,train_loader2, val_loader2 = prepare_training_data(batch_size,n_graphs)\n","\n","    # Training loop\n","    for data1,data2 in zip(train_loader1,train_loader2):\n","      optimizer.zero_grad()\n","      new_y1 = data1.y.reshape(batch_size,n_graphs).T\n","      new_y2 = data1.y.reshape(batch_size,n_graphs).T\n","      real_corr = spearman_corrcoef(new_y1, new_y2)\n","\n","      #Siamese Network corr\n","      pred_corr = model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n","                        data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n","      loss = criterion(pred_corr, real_corr.to(device))\n","      loss = loss.to(device)\n","      loss.backward()\n","      optimizer.step()\n","\n","    # Validation loss\n","    val_loss = 0\n","    print(f'len_val_loader:{len(val_loader1)}')\n","    for data1,data2 in zip(val_loader1,val_loader2):\n","        with torch.no_grad():\n","            new_y1 = data1.y.reshape(batch_size,n_graphs).T\n","            new_y2 = data1.y.reshape(batch_size,n_graphs).T\n","            real_corr = spearman_corrcoef(new_y1, new_y2)\n","            pred_corr = model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n","                              data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n","            real_corr = spearman_corrcoef(data1.y,data2.y)\n","            val_loss += criterion(pred_corr, real_corr.to(device)).item()\n","\n","    return val_loss / len(val_loader1)\n","\n","\n","def train_best_GNN_Siamese(study_best_params,num_node_features):\n","\n","    batch_size = study_best_params['batch_size']\n","    n_graphs = study_best_params['n_graphs']\n","    gat_heads = study_best_params['gat_heads']\n","    conv_hidden_channels=[study_best_params[f'conv_hidden_channels_{i}'] for i in range(study_best_params['num_conv_hidden_layers'])]\n","    activation=study_best_params['activation']\n","    readout_layer=study_best_params['readout_layer']\n","    criterion = study_best_params['criterion']\n","    lr = study_best_params['lr']\n","\n","    train_loader1, val_loader1,train_loader2, val_loader2 = prepare_training_data(study.best_params['batch_size'],study.best_params['n_graphs'])\n","\n","    spearman_corrcoef = DifferentialSpearmanCorrCoef(num_outputs=batch_size)\n","\n","    # Create the best model\n","    best_model = GNNSiamese(conv_hidden_channels,\n","                                  activation,\n","                                  batch_size,\n","                                  readout_layer,\n","                                  num_node_features,\n","                                  n_graphs,\n","                                  gat_heads).to(device)\n","\n","\n","    # Training loop\n","    optimizer = torch.optim.Adam(best_model.parameters(), lr=lr)\n","\n","    for epoch in range(study_best_params['num_epochs']):\n","      for data1,data2 in zip(train_loader1,train_loader2): #data1.x.shape = [n_graphs * batch_size, num_node_features]\n","          optimizer.zero_grad()\n","          #Real corr\n","          new_y1 = data1.y.reshape(batch_size,n_graphs).T\n","          new_y2 = data1.y.reshape(batch_size,n_graphs).T\n","          real_corr = spearman_corrcoef(new_y1, new_y2)\n","\n","          #Siamese Network corr\n","          pred_corr = best_model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n","                                data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n","          loss = criterion(pred_corr, real_corr.to(device))\n","          loss.backward()\n","          optimizer.step()\n","\n","\n","      for data1,data2 in zip(val_loader1,val_loader2):\n","        with torch.no_grad():\n","          for data1,data2 in zip(train_loader1,train_loader2): #data1.x.shape = [n_graphs * batch_size, num_node_features]\n","            #Real corr\n","            new_y1 = data1.y.reshape(batch_size,n_graphs).T\n","            new_y2 = data2.y.reshape(batch_size,n_graphs).T\n","            real_corr = spearman_corrcoef(new_y1, new_y2)\n","\n","            #Siamese Network corr\n","            pred_corr = best_model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n","                                  data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n","            val_loss = criterion(pred_corr, real_corr.to(device))\n","      print(f'epoch {epoch +1} done!')\n","\n","\n","    parameters ={\"batch_size\":batch_size,\"n_graphs\":n_graphs,'conv_hidden_channels':conv_hidden_channels, 'activation':activation,'readout_layer':readout_layer, 'num_node_features':num_node_features,'gat_heads':gat_heads}\n","    torch.save({\"model_state_dict\": best_model.state_dict(),\"params\":parameters},f'/graph_correlation/data/GNN_siamese_best_model_state_dict.pth')\n","\n","    return best_model\n","\n","def prepare_training_data(batch_size,n_graphs):\n","    dataset1 = torch.load(f'/graph_correlation/data/train/train_set1.pth')\n","    dataset2 = torch.load(f'/graph_correlation/data/train/train_set2.pth')\n","    scaler = joblib.load( f'/graph_correlation/data/scaler.pkl')\n","\n","    for data1,data2 in zip(dataset1,dataset2):\n","      data1.x = torch.tensor(scaler.transform(data1.x)).to(torch.float32)\n","      data2.x = torch.tensor(scaler.transform(data2.x)).to(torch.float32)\n","\n","    train_size = int(0.8 * len(dataset1))\n","    val_size = len(dataset1) - train_size\n","    train_dataset1, val_dataset1 = random_split(dataset1, [train_size, val_size])\n","    train_dataset2, val_dataset2 = random_split(dataset2, [train_size, val_size])\n","    train_loader1 = DataLoader(train_dataset1, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n","    train_loader2 = DataLoader(train_dataset2, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n","    val_loader1 = DataLoader(val_dataset1, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n","    val_loader2 = DataLoader(val_dataset2, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n","\n","    return train_loader1, val_loader1,train_loader2, val_loader2\n"],"metadata":{"id":"5LwZtQJk8KZ_","executionInfo":{"status":"ok","timestamp":1738939495181,"user_tz":180,"elapsed":92,"user":{"displayName":"Juliana Vizotto","userId":"07160986872486061354"}}},"id":"5LwZtQJk8KZ_","execution_count":13,"outputs":[]},{"id":"56f8992b-225a-4b67-a408-95425f93e245","cell_type":"markdown","source":["-------\n","\n","### Main\n","\n","Runs auxiliar functions to train the GNN-Siamese model. Returns and saves best model"],"metadata":{"id":"56f8992b-225a-4b67-a408-95425f93e245"}},{"cell_type":"code","source":["# Run Optuna optimization\n","study = optuna.create_study(direction='minimize')\n","study.optimize(lambda trial: objective(trial,num_layers,n_graphs,batch_size,lr,activation_name,criterion_name,num_epochs,gat_heads,readout_layer,num_node_features), n_trials=num_trails)\n","\n","#Train GNN-Siamese best model\n","study_best_params = study.best_params\n","GNN_Siamese = train_best_GNN_Siamese(study_best_params,num_node_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5Mz4F1B8qA5","executionInfo":{"status":"ok","timestamp":1738937750969,"user_tz":180,"elapsed":16140,"user":{"displayName":"Juliana Vizotto","userId":"07160986872486061354"}},"outputId":"28b865b9-5a5b-4795-d265-547245cc8044"},"id":"D5Mz4F1B8qA5","execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-07 14:15:34,669] A new study created in memory with name: no-name-d31978ac-76a6-483d-8b6b-fbe1d309ce74\n","<ipython-input-1-1593e045d32e>:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  lr = trial.suggest_loguniform('lr', lr[0], lr[1])\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains Tanh() which is of type Tanh.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ReLU() which is of type ReLU.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains Sigmoid() which is of type Sigmoid.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains MSELoss() which is of type MSELoss.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains L1Loss() which is of type L1Loss.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <function global_mean_pool at 0x78f31af5b920> which is of type function.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <function global_max_pool at 0x78f31af5b9c0> which is of type function.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <function global_add_pool at 0x78f31af5b880> which is of type function.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n"]},{"output_type":"stream","name":"stdout","text":["1\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-1593e045d32e>:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dataset1 = torch.load(f'/content/drive/MyDrive/TCC_KIKI/repo/data/train/train_set1.pth')\n","<ipython-input-1-1593e045d32e>:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dataset2 = torch.load(f'/content/drive/MyDrive/TCC_KIKI/repo/data/train/train_set2.pth')\n","/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["len_val_loader:42\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-07 14:15:47,177] Trial 0 finished with value: 1.3203984267477478 and parameters: {'n_graphs': 2, 'num_conv_hidden_layers': 2, 'conv_hidden_channels_0': 10, 'conv_hidden_channels_1': 7, 'batch_size': 1, 'lr': 0.0024307886401236465, 'activation': Sigmoid(), 'criterion': MSELoss(), 'num_epochs': 1, 'gat_heads': 1, 'readout_layer': <function global_add_pool at 0x78f31af5b880>}. Best is trial 0 with value: 1.3203984267477478.\n","<ipython-input-1-1593e045d32e>:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  lr = trial.suggest_loguniform('lr', lr[0], lr[1])\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains Tanh() which is of type Tanh.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ReLU() which is of type ReLU.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains Sigmoid() which is of type Sigmoid.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains MSELoss() which is of type MSELoss.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains L1Loss() which is of type L1Loss.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <function global_mean_pool at 0x78f31af5b920> which is of type function.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <function global_max_pool at 0x78f31af5b9c0> which is of type function.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <function global_add_pool at 0x78f31af5b880> which is of type function.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n","<ipython-input-1-1593e045d32e>:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dataset1 = torch.load(f'/content/drive/MyDrive/TCC_KIKI/repo/data/train/train_set1.pth')\n","<ipython-input-1-1593e045d32e>:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dataset2 = torch.load(f'/content/drive/MyDrive/TCC_KIKI/repo/data/train/train_set2.pth')\n"]},{"output_type":"stream","name":"stdout","text":["1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["len_val_loader:42\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-07 14:15:50,843] Trial 1 finished with value: 1.7018990370195846 and parameters: {'n_graphs': 2, 'num_conv_hidden_layers': 1, 'conv_hidden_channels_0': 11, 'batch_size': 1, 'lr': 0.0011167843201319064, 'activation': Tanh(), 'criterion': MSELoss(), 'num_epochs': 1, 'gat_heads': 1, 'readout_layer': <function global_max_pool at 0x78f31af5b9c0>}. Best is trial 0 with value: 1.3203984267477478.\n"]}]}]}