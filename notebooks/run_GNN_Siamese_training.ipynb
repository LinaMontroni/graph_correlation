{
  "metadata": {
    "kernelspec": {
      "name": "SQLite",
      "display_name": "SQLite",
      "language": "sql"
    },
    "language_info": {
      "codemirror_mode": "sql",
      "file_extension": "",
      "mimetype": "",
      "name": "sql",
      "version": "3.32.3"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "d1511e42-9d6e-41b7-9fbe-656ea109b87d",
      "cell_type": "markdown",
      "source": "# Train GNN-Siamese model\n\nUses Optuna hyper-parameter optmization for finding the best GNN-Siamese architecture. \nTrain and save best model.\n\n--------------------",
      "metadata": {}
    },
    {
      "id": "e135e71b-f39a-46e5-9977-266c67dc3be7",
      "cell_type": "markdown",
      "source": "### Imports",
      "metadata": {}
    },
    {
      "id": "86dde6f7-faab-4931-9b49-5069ed766220",
      "cell_type": "code",
      "source": "import os\nimport src.models.GNN_Siamese\nfrom GNN_Siamese import DifferentialSpearmanCorrCoef, GNNSiamese\nimport optuna",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ecab7f1f-9aaf-436e-9e9a-aa03590f5b6e",
      "cell_type": "markdown",
      "source": "---------------\n\n### Define the hyper-parameters range for Optuna trial\n",
      "metadata": {}
    },
    {
      "id": "f6bf3637-9e02-4c6e-b481-e7faefe4bb0f",
      "cell_type": "code",
      "source": "num_layers=[1,3] #Number of layers suggest for optuna trial. num_layers[0]:min, num_layers[1]:max\nbatch_size=[10,20,30,40,50] #Sizes of batch sizes suggest for optuna trial\nlr=[1e-4, 1e-2] #Learning rate suggest for optuna trial. num_layers[0]:min, num_layers[1]:max\nactivation_name=[torch.nn.Tanh(), torch.nn.ReLU(), torch.nn.Sigmoid()] #Activation functions suggest for optuna trial.\ncriterion_name=[torch.nn.MSELoss(), torch.nn.L1Loss()] #Loss functions suggest for optuna trial.\nnum_epochs=[5,50] # Number of epochs suggest for optuna trial\ngat_heads=[1,2,3,4,5] #Number of gat heads suggest for optuna trial\nreadout_layer=[nn.global_mean_pool,nn.global_max_pool,nn.global_add_pool] #Pooling layers suggest for optuna trial.\nnum_trails=50 #Number of trials optuna should run",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "59b871d8-c5c9-40a4-a31e-9c427e48826f",
      "cell_type": "markdown",
      "source": "### Define auxiliar functions \n\n- Optuna objective trial: runs hyper-parameter optmization given the defined ranges above\n- GNN-Siamese training function: train the full GNN-Siamese model with the parameters for the best model returned by optuna optimization\n- Prepare training data: generator fited torch geometric DataLoader for training",
      "metadata": {}
    },
    {
      "id": "f504d9cb-210e-4d3c-9019-1c4bd78826fc",
      "cell_type": "code",
      "source": "def objective(trial,num_layers,batch_size,lr,activation_name,criterion_name,num_epochs,gat_heads,readout_layer,\n             train_loader1, val_loader1,train_loader2, val_loader2):\n    # Suggest hyperparameters\n    num_node_features = 13\n    num_layers = trial.suggest_int('num_conv_hidden_layers', num_layers[0], num_layers[1])\n    conv_hidden_channels = []\n    min_val = 9\n    for i in range(num_layers):\n        # Suggest a decreasing value for each subsequent layer\n        max_val = 12 if i == 0 else conv_hidden_channels[-1] - 1\n        if max_val < min_val:\n            raise ValueError(\"Cannot generate strictly decreasing channels within the range.\")\n        conv_hidden_channels.append(trial.suggest_int(f'conv_hidden_channels_{i}', min_val, max_val))\n        min_val = min_val - 2\n        \n    batch_size = trial.suggest_categorical('batch_size', batch_size)\n    lr = trial.suggest_loguniform('lr', lr[0], lr[1])\n    activation_name = trial.suggest_categorical('activation', activation_name)\n    criterion = trial.suggest_categorical('criterion', criterion_name)\n    num_epochs = trial.suggest_int('num_epochs', num_epochs[0], num_epochs[1])\n    gat_heads = trial.suggest_categorical('gat_heads', gat_heads)\n    readout_layer = trial.suggest_categorical('readout_layer',readout_layer)\n    \n    spearman_corrcoef = DifferentialSpearmanCorrCoef(num_outputs=batch_size)\n\n    #Instanciate Model\n    model = GNNSiamese(conv_hidden_channels=conv_hidden_channels,\n                           activation=activation_name,\n                           batch_size=batch_size,\n                           readout_layer=readout_layer,\n                           num_node_features=num_node_features,\n                           n_graphs=n_graphs,\n                           gat_heads=gat_heads).to(device)\n\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # Training loop\n    for data1,data2 in zip(train_loader1,train_loader2):\n      optimizer.zero_grad()\n      new_y1 = data1.y.reshape(batch_size,n_graphs).T\n      new_y2 = data1.y.reshape(batch_size,n_graphs).T\n      real_corr = spearman_corrcoef(new_y1, new_y2)\n\n      #Siamese Network corr\n      pred_corr = model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n                        data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n      loss = criterion(pred_corr, real_corr.to(device))\n      loss = loss.to(device)\n      loss.backward()\n      optimizer.step()\n\n    # Validation loss\n    val_loss = 0\n    for data1,data2 in zip(val_loader1,val_loader2):\n        with torch.no_grad():\n            new_y1 = data1.y.reshape(batch_size,n_graphs).T\n            new_y2 = data1.y.reshape(batch_size,n_graphs).T\n            real_corr = spearman_corrcoef(new_y1, new_y2)\n            pred_corr = model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n                              data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n            real_corr = spearman_corrcoef(data1.y,data2.y)\n            val_loss += criterion(pred_corr, real_corr.to(device)).item()\n\n    return val_loss / len(val_loader1)\n\n\ndef train_best_GNN_Siamese(study_best_params,train_loader1, val_loader1,train_loader2, val_loader2):\n    \n    batch_size = study_best_params['batch_size']\n    n_graphs = study_best_params['n_graphs']\n    gat_heads = study_best_params['gat_heads']\n    conv_hidden_channels=[study_best_params[f'conv_hidden_channels_{i}'] for i in range(study_best_params['num_conv_hidden_layers'])]\n    activation=activations[study_best_params['activation']]\n    readout_layer=readout[study_best_params['readout_layer']]\n    criterion = criteria[study_best_params['criterion']]\n    lr = study_best_params['lr']\n    \n    spearman_corrcoef = DifferentialSpearmanCorrCoef(num_outputs=batch_size*n_graphs)\n    \n    # Create the best model\n    best_model = GCNSimpleSiamese(conv_hidden_channels,\n                                  activation,\n                                  batch_size,\n                                  readout_layer,\n                                  num_node_features,\n                                  n_graphs,\n                                  gat_heads).to(device)\n    \n    \n    # Training loop\n    optimizer = torch.optim.Adam(best_model.parameters(), lr=lr)\n    \n    train_losses = []\n    val_losses = []\n    spearman_corrcoef = NewSpearmanCorrCoef(num_outputs=batch_size)\n    t_start_time = time.time()\n    for epoch in range(study_best_params['num_epochs']):\n      for data1,data2 in zip(train_loader1,train_loader2): #data1.x.shape = [n_graphs * batch_size, num_node_features]\n          optimizer.zero_grad()\n          #Real corr\n          new_y1 = data1.y.reshape(batch_size,n_graphs).T\n          new_y2 = data1.y.reshape(batch_size,n_graphs).T\n          real_corr = spearman_corrcoef(new_y1, new_y2)\n    \n          #Siamese Network corr\n          pred_corr = best_model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n                                data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n          loss = criterion(pred_corr, real_corr.to(device))\n          loss.backward()\n          optimizer.step()\n      \n      train_losses.append(loss.item())\n    \n      for data1,data2 in zip(val_loader1,val_loader2):\n          with torch.no_grad():\n              for data1,data2 in zip(train_loader1,train_loader2): #data1.x.shape = [n_graphs * batch_size, num_node_features]\n                #Real corr\n                new_y1 = data1.y.reshape(batch_size,n_graphs).T\n                new_y2 = data2.y.reshape(batch_size,n_graphs).T\n                real_corr = spearman_corrcoef(new_y1, new_y2)\n    \n                #Siamese Network corr\n                pred_corr = best_model(data1.x.to(device), data1.edge_index.to(device), data1.batch.to(device),\n                                       data2.x.to(device), data2.edge_index.to(device), data2.batch.to(device))\n                val_loss = criterion(pred_corr, real_corr.to(device))\n       val_losses.append(val_loss.item())\n\n        \n    parameters ={\"batch_size\":batch_size,\"n_graphs\":n_graphs,'conv_hidden_channels':conv_hidden_channels,'lin_hidden_channels':lin_hidden_channels, 'activation':activation, 'layer_type':layer_type,'readout_layer':readout_layer, 'num_node_features':num_node_features,'gat_heads':gat_heads}\n    torch.save({\"model_state_dict\": best_model.state_dict(),\"params\":parameters},f'/models/GNN_siamese_best_model_state_dict.pth')\n\n    return best_model\n\ndef prepare_training_data():\n    dataset1 = torch.load(f'/data/train/train_set1.pkl')\n    dataset2 = torch.load(f'/data/train/train_set2.pkl')\n    scaler = joblib.load( f'/data/scaler.pkl')\n    for data1,data2 in zip(dataset1,dataset2):\n      data1.x = torch.tensor(scaler.transform(data1.x)).to(torch.float32)\n      data2.x = torch.tensor(scaler.transform(data2.x)).to(torch.float32)\n        \n    train_size = int(0.8 * len(dataset1))\n    val_size = len(dataset1) - train_size\n    train_dataset1, val_dataset1 = random_split(dataset1, [train_size, val_size])\n    train_dataset2, val_dataset2 = random_split(dataset2, [train_size, val_size])\n    train_loader1 = DataLoader(train_dataset1, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n    train_loader2 = DataLoader(train_dataset2, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n    val_loader1 = DataLoader(val_dataset1, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n    val_loader2 = DataLoader(val_dataset2, batch_size=batch_size*n_graphs, shuffle=True,drop_last=True)\n    ÃŸ\n    return train_loader1, val_loader1,train_loader2, val_loader2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "56f8992b-225a-4b67-a408-95425f93e245",
      "cell_type": "markdown",
      "source": "-------\n\n### Main\n\nRuns auxiliar functions to train the GNN-Siamese model. Returns and saves best model",
      "metadata": {}
    },
    {
      "id": "2cc44aa6-627a-41b8-96d4-320f5efd4d3b",
      "cell_type": "code",
      "source": "# Generate DataLoaders\ntrain_loader1, val_loader1,train_loader2, val_loader2 = prepare_training_data()\n\n# Run Optuna optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(lambda trial: objective(trial,num_layers,batch_size,lr,activation_name,criterion_name,num_epochs,gat_heads,readout_layer,\n                                       train_loader1, val_loader1,train_loader2, val_loader2), n_trials=num_trails)\nstudy_best_params = study.best_params\nGNN_Siamese = train_best_GNN_Siamese(study_best_params,train_loader1, val_loader1,train_loader2, val_loader2)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}